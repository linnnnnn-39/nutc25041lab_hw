import json
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser  # 1. å¿…é ˆåŒ¯å…¥é€™å€‹

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="your_api_key_here", 
    model="google/gemma-3-27b-it",
    temperature=0
)

# 2. ä¿®æ­£å·¥å…·å®šç¾©
@tool
def generate_tech_summary(article_content: str):
    """
    é€™æ˜¯ä¸€å€‹ç§‘æŠ€æ–‡ç« æ‘˜è¦å·¥å…·ã€‚ç•¶ä½¿ç”¨è€…æä¾›çš„å…§å®¹æ¶‰åŠç§‘æŠ€ã€AIã€ç¡¬é«”æˆ–è»Ÿé«”æ–°èæ™‚ï¼Œè«‹ä½¿ç”¨æ­¤å·¥å…·é€²è¡Œæ‘˜è¦ã€‚
    """
    # ä¿®æ­£ï¼šPrompt åˆ—è¡¨éºæ¼é€—è™Ÿï¼Œä¸” text æ‡‰å°æ‡‰ invoke çš„ key
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹ç§‘æŠ€ç­†è¨˜åŠ©æ‰‹ï¼Œè«‹ç”¨ç°¡çŸ­çš„ä¸‰å€‹é‡é»æ‘˜è¦ä»¥ä¸‹å…§å®¹ã€‚"),
        ("user", "{text}")
    ])
    
    # ä¿®æ­£ï¼šå·¥å…·å…§éƒ¨ä¹Ÿè¦èƒ½å­˜å–åˆ°å¤–å±¤çš„ llm
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({"text": article_content})
    return result

# 3. ç¶å®šå·¥å…·
llm_with_tools = llm.bind_tools([generate_tech_summary])

# 4. è·¯ç”±æç¤ºè©
router_prompt = ChatPromptTemplate.from_messages([
    ("user", "{user_input}")
])

while True:
    user_input = input("User: ")
    
    if user_input.lower() in ["exit", "q"]:
        print("Bye!")
        break
    
    # 5. ä¿®æ­£ï¼šé€™è£¡çš„ invoke åƒæ•¸å¿…é ˆè·Ÿ router_prompt çš„è®Šæ•¸å {user_input} ä¸€è‡´
    ai_msg = (router_prompt | llm_with_tools).invoke({"user_input": user_input})
    
    # åˆ¤æ–·é‚è¼¯
    if ai_msg.tool_calls:
        print(f"âœ… [æ±ºç­–] åˆ¤æ–·ç‚ºç§‘æŠ€æ–‡ç« ")
        # å–å¾—å·¥å…·åƒæ•¸
        tool_args = ai_msg.tool_calls[0]['args']
        
        # åŸ·è¡Œå·¥å…·
        final_result = generate_tech_summary.invoke(tool_args)
        print(f"ğŸ“„ [åŸ·è¡Œçµæœ]:\n{final_result}")
    else:
        print(f"âŒ [æ±ºç­–] åˆ¤æ–·ç‚ºé–’èŠ")
        print(f"ğŸ’¬ [AI å›æ‡‰]: {ai_msg.content}")