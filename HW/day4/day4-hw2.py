import os
import requests
import operator
from typing import Annotated, List, TypedDict, Literal
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END, START

# --- 1. é…ç½® ---
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# å»ºç«‹ LLM é€£ç·š
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="YOUR_API_KEY", 
    model="google/gemma-3-27b-it",
    temperature=0.3,
    timeout=20.0  # é¿å…ç„¡é™ç­‰å¾…
)

class AgentState(TypedDict):
    input: str
    knowledge_base: Annotated[List[str], operator.add]
    search_count: int
    next_step: str 
    current_plan: str
    final_response: str

# --- 2. ç¯€é»åŠŸèƒ½å®šç¾© ---

def router_node(state: AgentState):
    """åˆ¤æ–·è©²èµ°å¿«é€Ÿå›è¦†é‚„æ˜¯æœå°‹"""
    print("--- [Node] è·¯ç”±åˆ¤æ–· ---")
    text = state["input"].strip().lower()
    # é—œéµå­—æ””æˆªï¼šåŒ…å«é€™äº›å­—çœ¼ç›´æ¥èµ°æœå°‹
    realtime_keywords = ["è‚¡åƒ¹", "ç¾åœ¨", "æ–°è", "å ±åƒ¹", "å¤§è·Œ", "ç‚ºä½•", "ç‚ºä»€éº¼"]
    if any(kw in text for kw in realtime_keywords):
        return {"next_step": "search_path"}
    
    try:
        prompt = f"åˆ¤æ–·å•é¡Œæ˜¯å¦éœ€è¦å³æ™‚è³‡è¨Šæˆ–äº‹å¯¦æŸ¥è©¢ï¼š'{text}'ã€‚è‹¥æ˜¯é–’èŠå›å‚³ FASTï¼Œå¦å‰‡å›å‚³ SEARCHã€‚åªå‡†å›å‚³å–®å­—ã€‚"
        res = llm.invoke(prompt).content.upper()
        return {"next_step": "fast_path" if "FAST" in res else "search_path"}
    except:
        return {"next_step": "search_path"}

def fast_answer_node(state: AgentState):
    """å¿«é€Ÿå›è¦†é–’èŠ"""
    print("--- [Node] å¿«é€Ÿé€šé“ ---")
    try:
        res = llm.invoke(state["input"]).content
        return {"final_response": res}
    except:
        return {"final_response": "ä½ å¥½ï¼ç›®å‰æˆ‘æœ‰é»é€£ç·šå›°é›£ï¼Œä½†å¾ˆé«˜èˆˆè¦‹åˆ°ä½ ã€‚"}

def query_gen_node(state: AgentState):
    """ç”Ÿæˆæˆ–å„ªåŒ–æœå°‹é—œéµå­—"""
    print(f"--- [Node] ç”Ÿæˆæœå°‹è¨ˆç•« (ç¬¬ {state['search_count']+1} æ¬¡) ---")
    query = state["input"]
    try:
        # å¦‚æœæ˜¯ç¬¬äºŒæ¬¡æœå°‹ï¼Œå˜—è©¦è®Šæ›é—œéµå­—
        prompt_text = f"å„ªåŒ–æœå°‹é—œéµå­—ï¼š'{query}'" if state['search_count'] == 0 else f"æ›å€‹æ–¹å¼æœï¼š'{query}' çš„åŸå› èˆ‡åˆ†æ"
        res = llm.invoke(f"{prompt_text}ã€‚åªå›å‚³é—œéµå­—ï¼Œä¸è¦å»¢è©±ã€‚").content.strip()
        if res and len(res) > 1: query = res
    except:
        if "å°æ©Ÿé›»" in query: query = "å°ç©é›» 2330 è‚¡åƒ¹"
    return {"current_plan": query}

def search_tool_node(state: AgentState):
    """åŸ·è¡Œå¯¦éš›è¯ç¶²æœå°‹"""
    print(f"--- [Node] åŸ·è¡Œæœå°‹: {state['current_plan']} ---")
    try:
        r = requests.get(SEARXNG_URL, params={"q": state["current_plan"], "format": "json"}, timeout=15)
        results = r.json().get('results', [])
        if not results:
            return {"knowledge_base": ["(æœªæ‰¾åˆ°æœå°‹çµæœ)"], "search_count": state["search_count"] + 1}
        
        info = "\n".join([f"ä¾†æº: {res['url']}\nå…§å®¹: {res.get('content')}" for res in results[:2]])
        return {"knowledge_base": [info], "search_count": state["search_count"] + 1}
    except Exception as e:
        return {"knowledge_base": [f"æœå°‹å¤±æ•—: {e}"], "search_count": state["search_count"] + 1}

def planner_node(state: AgentState):
    """åˆ¤æ–·è³‡æ–™æ˜¯å¦è¶³å¤ æˆ–é”åˆ°æ¬¡æ•¸ä¸Šé™"""
    print(f"--- [Node] å¯©æŸ¥æ•¸æ“š ---")
    if state["search_count"] >= 2 or len("".join(state["knowledge_base"])) > 100:
        return {"next_step": "complete"}
    return {"next_step": "continue"}

def final_answer_node(state: AgentState):
    """æ•´åˆè³‡æ–™çµ¦å‡ºæœ€çµ‚ç­”æ¡ˆ"""
    print("--- [Node] ç”Ÿæˆæœ€çµ‚å›è¦† ---")
    context = "".join(state["knowledge_base"])
    prompt = f"æ ¹æ“šè³‡æ–™å›ç­”å•é¡Œï¼š{state['input']}\nè³‡æ–™ï¼š\n{context}\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
    try:
        res = llm.invoke(prompt).content
        if not res or len(res) < 5: raise ValueError("å›è¦†ç•°å¸¸")
        return {"final_response": res}
    except:
        # æ•‘æ´æ©Ÿåˆ¶ï¼šLLM å£æ‰æ™‚ç›´æ¥çµ¦æœå°‹æ‘˜è¦
        return {"final_response": f"ï¼ˆå³æ™‚æ‘˜è¦ï¼‰ï¼š\n{context[:500]}..."}

# --- 3. å»ºæ§‹æ•´åˆåœ–å½¢çµæ§‹ ---

workflow = StateGraph(AgentState)

# æ–°å¢ç¯€é»
workflow.add_node("router", router_node)
workflow.add_node("fast_answer", fast_answer_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("planner", planner_node)
workflow.add_node("final_answer", final_answer_node)

# è¨­å®šé€£ç·š
workflow.add_edge(START, "router")

# æ¢ä»¶è·¯ç”±
workflow.add_conditional_edges(
    "router",
    lambda x: x["next_step"],
    {"fast_path": "fast_answer", "search_path": "planner"}
)

workflow.add_conditional_edges(
    "planner",
    lambda x: x["next_step"],
    {"continue": "query_gen", "complete": "final_answer"}
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")
workflow.add_edge("fast_answer", END)
workflow.add_edge("final_answer", END)

app = workflow.compile()

# --- 4. æ¸¬è©¦é‹è¡Œ ---
if __name__ == "__main__":
    # åˆ—å°çµæ§‹åœ– (ASCII)
    try:
        app.get_graph().print_ascii()
    except:
        print("ç„¡æ³•åˆ—å° ASCII åœ–ï¼Œä½†æµç¨‹å·²å°±ç·’ã€‚")

    print("\n--- ç³»çµ±å•Ÿå‹• (è¼¸å…¥ 'exit' é€€å‡º) ---")
    while True:
        user_input = input("ğŸ‘¤ æå•: ")
        if user_input.lower() == 'exit': break
        
        for output in app.stream({"input": user_input, "knowledge_base": [], "search_count": 0}):
            for node, data in output.items():
                if "final_response" in data:
                    print(f"\nğŸ¯ AIï¼š\n{data['final_response']}\n")